{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate SAE Features to Compare Memorized vs NonMemorized Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/Code/LLMSAEMemorization/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 119.97it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model using TransformerLens\n",
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-3.1-8B\", device=device, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# No need for separate tokenizer as it's included in the HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAEs for all layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:52<00:00, 13.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading SAEs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the SAEs for all layers\n",
    "sae_layers = range(28, 32)\n",
    "release = \"llama_scope_lxr_8x\"\n",
    "saes = []\n",
    "print(\"Loading SAEs for all layers...\")\n",
    "for layer in tqdm(sae_layers):\n",
    "    sae_id = f\"l{layer}r_8x\"\n",
    "    sae = SAE.from_pretrained(release, sae_id)[0]\n",
    "    sae = sae.to(device)\n",
    "    saes.append(sae)\n",
    "print(\"Finished loading SAEs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Memorization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total perfect matches: 26\n",
      "Total non-perfect matches: 83\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('memorization_results_L50_O50.csv')\n",
    "\n",
    "# Split into perfect and non-perfect matches\n",
    "perfect_matches = df[df['perfect_match'] == True]\n",
    "non_perfect_matches = df[(df['perfect_match'] == False) & (df['matching_tokens'] == 0)]\n",
    "\n",
    "# Sample from each\n",
    "num_samples = 20\n",
    "perfect_sample = perfect_matches.sample(n=num_samples, random_state=random_seed)\n",
    "non_perfect_sample = non_perfect_matches.sample(n=num_samples, random_state=random_seed)\n",
    "\n",
    "print(f\"Total perfect matches: {len(perfect_matches)}\")\n",
    "print(f\"Total non-perfect matches: {len(non_perfect_matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get SAE Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(text_input, text_output):\n",
    "    \"\"\"Get SAE activations for a given input-output pair using TransformerLens\"\"\"\n",
    "    # Concatenate input and output\n",
    "    full_text = text_input + text_output\n",
    "    \n",
    "    # Tokenize and get model activations using run_with_cache\n",
    "    tokens = model.to_tokens(full_text, prepend_bos=False)\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    # Get layer activations for all layers\n",
    "    all_layer_acts = {}\n",
    "    for layer, sae in zip(sae_layers, saes):\n",
    "        # Get layer activations\n",
    "        layer_acts = cache['resid_pre', layer]\n",
    "        \n",
    "        # Get SAE activations\n",
    "        sae_acts = sae.encode(layer_acts)\n",
    "        \n",
    "        # Only keep activations for output tokens (last 50)\n",
    "        output_acts = sae_acts[:, -50:, :]\n",
    "        \n",
    "        all_layer_acts[layer] = output_acts.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "    return all_layer_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing perfect matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:02,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing non-perfect matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:01, 11.49it/s]\n"
     ]
    }
   ],
   "source": [
    "if 'perfect_acts_stacked' in locals():\n",
    "    del perfect_acts_stacked\n",
    "if 'non_perfect_acts_stacked' in locals():\n",
    "    del non_perfect_acts_stacked\n",
    "\n",
    "# Get activations for perfect matches\n",
    "perfect_activations_by_layer = {layer: [] for layer in sae_layers}\n",
    "print(\"Processing perfect matches...\")\n",
    "for _, row in tqdm(perfect_sample.iterrows()):\n",
    "    acts_dict = get_activations(row['input_text'], row['generated_continuation_text'])\n",
    "    for layer in sae_layers:\n",
    "        perfect_activations_by_layer[layer].append(acts_dict[layer])\n",
    "\n",
    "# Get activations for non-perfect matches\n",
    "non_perfect_activations_by_layer = {layer: [] for layer in sae_layers}\n",
    "print(\"\\nProcessing non-perfect matches...\")\n",
    "for _, row in tqdm(non_perfect_sample.iterrows()):\n",
    "    acts_dict = get_activations(row['input_text'], row['generated_continuation_text'])\n",
    "    for layer in sae_layers:\n",
    "        non_perfect_activations_by_layer[layer].append(acts_dict[layer])\n",
    "\n",
    "# Stack all activations for each layer\n",
    "perfect_acts_stacked = {\n",
    "    layer: torch.stack(acts, dim=0) \n",
    "    for layer, acts in perfect_activations_by_layer.items()\n",
    "}\n",
    "non_perfect_acts_stacked = {\n",
    "    layer: torch.stack(acts, dim=0)\n",
    "    for layer, acts in non_perfect_activations_by_layer.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Activation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate statistics\n",
    "# def compute_stats(activations):\n",
    "#     \"\"\"Compute various statistics for the activations\"\"\"\n",
    "#     # L0 (number of active features)\n",
    "#     l0 = (activations > 0).float().sum(-1).mean().item()\n",
    "    \n",
    "#     # Mean activation when active\n",
    "#     mean_active = activations[activations > 0].mean().item()\n",
    "    \n",
    "#     # Max activation\n",
    "#     max_act = activations.max().item()\n",
    "    \n",
    "#     # Feature sparsity (fraction of features that never activate)\n",
    "#     feature_sparsity = ((activations > 0).sum(0) == 0).float().mean().item()\n",
    "    \n",
    "#     return {\n",
    "#         'L0 (avg active features)': l0,\n",
    "#         'Mean activation when active': mean_active,\n",
    "#         'Max activation': max_act,\n",
    "#         'Feature sparsity': feature_sparsity\n",
    "#     }\n",
    "\n",
    "# perfect_stats = compute_stats(perfect_acts_stacked)\n",
    "# non_perfect_stats = compute_stats(non_perfect_acts_stacked)\n",
    "\n",
    "# # Print statistics\n",
    "# print(\"Statistics for perfect matches:\")\n",
    "# for k, v in perfect_stats.items():\n",
    "#     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# print(\"\\nStatistics for non-perfect matches:\")\n",
    "# for k, v in non_perfect_stats.items():\n",
    "#     print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot L0 histograms\n",
    "# l0_perfect = (perfect_acts_stacked > 0).float().sum(-1).cpu().numpy()\n",
    "# l0_non_perfect = (non_perfect_acts_stacked > 0).float().sum(-1).cpu().numpy()\n",
    "\n",
    "# # Create a DataFrame for plotting\n",
    "# l0_data = pd.DataFrame({\n",
    "#     'L0': np.concatenate([l0_perfect, l0_non_perfect]),\n",
    "#     'Type': ['Perfect Match'] * len(l0_perfect) + ['Non-Perfect Match'] * len(l0_non_perfect)\n",
    "# })\n",
    "\n",
    "# # Plot histogram\n",
    "# fig = px.histogram(l0_data, x='L0', color='Type', barmode='overlay',\n",
    "#                   title='Distribution of Active Features (L0)',\n",
    "#                   labels={'L0': 'Number of Active Features', 'count': 'Frequency'})\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot activation magnitude distributions\n",
    "# act_perfect = perfect_acts_stacked[perfect_acts_stacked > 0].float().detach().cpu().numpy()\n",
    "# act_non_perfect = non_perfect_acts_stacked[non_perfect_acts_stacked > 0].float().detach().cpu().numpy()\n",
    "\n",
    "# # Create a DataFrame for plotting\n",
    "# act_data = pd.DataFrame({\n",
    "#     'Activation': np.concatenate([act_perfect, act_non_perfect]),\n",
    "#     'Type': ['Perfect Match'] * len(act_perfect) + ['Non-Perfect Match'] * len(act_non_perfect)\n",
    "# })\n",
    "\n",
    "# # Plot histogram\n",
    "# fig = px.histogram(act_data, x='Activation', color='Type', barmode='overlay',\n",
    "#                   title='Distribution of Activation Magnitudes',\n",
    "#                   labels={'Activation': 'Activation Value', 'count': 'Frequency'})\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SAE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing layer 28:\n",
      "    Feature  Perfect  Non-Perfect  Abs_Difference  Percent_Difference\n",
      "64     9966      511          795             284           35.723270\n",
      "35     4245      445          103             342           76.853933\n",
      "12    21044      432          323             109           25.231481\n",
      "29    24190      123            0             123          100.000000\n",
      "33    24462       63          290             227           78.275862\n",
      "58    11993       46          172             126           73.255814\n",
      "14    32063       11          192             181           94.270833\n",
      "\n",
      "Analyzing layer 29:\n",
      "    Feature  Perfect  Non-Perfect  Abs_Difference  Percent_Difference\n",
      "73    22526      485          820             335           40.853659\n",
      "21    16698      465          624             159           25.480769\n",
      "54    14008      277           74             203           73.285199\n",
      "24     8008      224          359             135           37.604457\n",
      "43     4238      189          295             106           35.932203\n",
      "45    11678      143            0             143          100.000000\n",
      "49     6313      142          473             331           69.978858\n",
      "16    32042      100          258             158           61.240310\n",
      "72     1021       78          232             154           66.379310\n",
      "50    23466       20          128             108           84.375000\n",
      "32    12386        6          109             103           94.495413\n",
      "\n",
      "Analyzing layer 30:\n",
      "    Feature  Perfect  Non-Perfect  Abs_Difference  Percent_Difference\n",
      "17    24133      466          831             365           43.922984\n",
      "33    27766      369           90             279           75.609756\n",
      "66    17891      287          544             257           47.242647\n",
      "8     25899      271          388             117           30.154639\n",
      "68    14054      232          427             195           45.667447\n",
      "21    15194      187            1             186           99.465241\n",
      "67    25828      178           71             107           60.112360\n",
      "62    14032      169            2             167           98.816568\n",
      "64    10457      164           58             106           64.634146\n",
      "43    21145      124          373             249           66.756032\n",
      "22    15707       91          418             327           78.229665\n",
      "73     9725       64          181             117           64.640884\n",
      "55    30647       24          140             116           82.857143\n",
      "26    26728       15          118             103           87.288136\n",
      "6     29480       14          123             109           88.617886\n",
      "40    25737       12          114             102           89.473684\n",
      "47    25768        2          120             118           98.333333\n",
      "\n",
      "Analyzing layer 31:\n",
      "    Feature  Perfect  Non-Perfect  Abs_Difference  Percent_Difference\n",
      "57     9926      765          932             167           17.918455\n",
      "46     2216      258          554             296           53.429603\n",
      "43    20372      172          325             153           47.076923\n",
      "8     21781      165           24             141           85.454545\n",
      "71     9965      111           10             101           90.990991\n",
      "20    10304       22          138             116           84.057971\n",
      "58    16586       11          130             119           91.538462\n",
      "54    17597       11          111             100           90.090090\n"
     ]
    }
   ],
   "source": [
    "num_top_features = 50\n",
    "\n",
    "# Analyze feature usage patterns\n",
    "def get_top_features(activations, n=num_top_features):\n",
    "    \"\"\"Get the most frequently activated features\"\"\"\n",
    "    # Compress activations across all samples first\n",
    "    compressed_acts = activations.reshape(-1, activations.shape[-1])\n",
    "    feature_counts = (compressed_acts > 0).float().sum(0)\n",
    "    top_features = torch.topk(feature_counts, n)\n",
    "    return top_features.indices.cpu().numpy(), top_features.values.cpu().numpy()\n",
    "\n",
    "# Since we're working with a dict now, we need to handle one layer at a time\n",
    "aggregate_feature_comparisons_tables = {}\n",
    "for layer in sae_layers:\n",
    "    print(f\"\\nAnalyzing layer {layer}:\")\n",
    "    \n",
    "    # Get the activation tensors for this layer\n",
    "    perfect_acts = perfect_acts_stacked[layer]\n",
    "    non_perfect_acts = non_perfect_acts_stacked[layer]\n",
    "\n",
    "    # Get top features for both types\n",
    "    perfect_top_idx, perfect_top_counts = get_top_features(perfect_acts)\n",
    "    non_perfect_top_idx, non_perfect_top_counts = get_top_features(non_perfect_acts)\n",
    "\n",
    "    # Create a set of all features that were activated in either type\n",
    "    overlap_features = set(perfect_top_idx.tolist()) | set(non_perfect_top_idx.tolist())\n",
    "\n",
    "    # For each feature, see how many times it was activated in each type\n",
    "    perfect_acts_flat = perfect_acts.reshape(-1, perfect_acts.shape[-1])\n",
    "    non_perfect_acts_flat = non_perfect_acts.reshape(-1, non_perfect_acts.shape[-1])\n",
    "    \n",
    "    feature_counts = (perfect_acts_flat > 0).float().sum(0).int().cpu().numpy()\n",
    "    feature_counts_non_perfect = (non_perfect_acts_flat > 0).float().sum(0).int().cpu().numpy()\n",
    "    \n",
    "    # Create a table to show all the features (even some that we only activated by one type)\n",
    "    feature_counts_table = pd.DataFrame({\n",
    "        'Feature': list(overlap_features),\n",
    "        'Perfect': feature_counts[list(overlap_features)],\n",
    "        'Non-Perfect': feature_counts_non_perfect[list(overlap_features)]\n",
    "    })\n",
    "    \n",
    "    # Add difference columns\n",
    "    feature_counts_table['Abs_Difference'] = abs(feature_counts_table['Perfect'] - feature_counts_table['Non-Perfect'])\n",
    "    feature_counts_table['Percent_Difference'] = (feature_counts_table['Abs_Difference'] / \n",
    "                                                feature_counts_table[['Perfect', 'Non-Perfect']].max(axis=1) * 100)\n",
    "    \n",
    "    # Filter for absolute difference >= 100\n",
    "    feature_counts_table = feature_counts_table[feature_counts_table['Abs_Difference'] >= 100]\n",
    "\n",
    "    # Sort the table by the number of activations in each type\n",
    "    feature_counts_table = feature_counts_table.sort_values(by=['Perfect', 'Non-Perfect'], ascending=False)\n",
    "\n",
    "    # Print the table\n",
    "    print(feature_counts_table)\n",
    "    aggregate_feature_comparisons_tables[layer] = feature_counts_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature counts for layer 28 to aggregate_feature_comparisons/feature_counts_layer_28.csv\n",
      "Saved feature counts for layer 29 to aggregate_feature_comparisons/feature_counts_layer_29.csv\n",
      "Saved feature counts for layer 30 to aggregate_feature_comparisons/feature_counts_layer_30.csv\n",
      "Saved feature counts for layer 31 to aggregate_feature_comparisons/feature_counts_layer_31.csv\n"
     ]
    }
   ],
   "source": [
    "# Create directory for feature comparison tables if it doesn't exist\n",
    "import os\n",
    "output_dir = \"aggregate_feature_comparisons\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for layer, feature_counts_table in aggregate_feature_comparisons_tables.items():\n",
    "    # Save the feature counts table for this layer\n",
    "    output_path = os.path.join(output_dir, f\"feature_counts_layer_{layer}.csv\")\n",
    "    feature_counts_table.to_csv(output_path, index=False)\n",
    "    print(f\"Saved feature counts for layer {layer} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
