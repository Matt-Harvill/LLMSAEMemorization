{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate SAE Features to Compare Memorized vs NonMemorized Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/Code/LLMSAEMemorization/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 58.46it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model using TransformerLens\n",
    "model = HookedTransformer.from_pretrained(\"meta-llama/Llama-3.1-8B\", device=device, torch_dtype=torch.bfloat16)\n",
    "\n",
    "# No need for separate tokenizer as it's included in the HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAEs for all layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:13<00:00,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading SAEs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the SAEs for all layers\n",
    "sae_layers = range(16, 20)\n",
    "release = \"llama_scope_lxr_8x\"\n",
    "saes = []\n",
    "print(\"Loading SAEs for all layers...\")\n",
    "for layer in tqdm(sae_layers):\n",
    "    sae_id = f\"l{layer}r_8x\"\n",
    "    sae = SAE.from_pretrained(release, sae_id)[0]\n",
    "    sae = sae.to(device)\n",
    "    saes.append(sae)\n",
    "print(\"Finished loading SAEs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Memorization Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total perfect matches: 26\n",
      "Total non-perfect matches: 83\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df = pd.read_csv('memorization_results_L50_O50.csv')\n",
    "\n",
    "# Split into perfect and non-perfect matches\n",
    "perfect_matches = df[df['perfect_match'] == True]\n",
    "non_perfect_matches = df[(df['perfect_match'] == False) & (df['matching_tokens'] == 0)]\n",
    "\n",
    "# Sample from each\n",
    "num_samples = 20\n",
    "perfect_sample = perfect_matches.sample(n=num_samples, random_state=random_seed)\n",
    "non_perfect_sample = non_perfect_matches.sample(n=num_samples, random_state=random_seed)\n",
    "\n",
    "print(f\"Total perfect matches: {len(perfect_matches)}\")\n",
    "print(f\"Total non-perfect matches: {len(non_perfect_matches)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get SAE Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(text_input, text_output):\n",
    "    \"\"\"Get SAE activations for a given input-output pair using TransformerLens\"\"\"\n",
    "    # Concatenate input and output\n",
    "    full_text = text_input + text_output\n",
    "    \n",
    "    # Tokenize and get model activations using run_with_cache\n",
    "    tokens = model.to_tokens(full_text, prepend_bos=False)\n",
    "    _, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    # Get layer activations for all layers\n",
    "    all_layer_acts = {}\n",
    "    for layer, sae in zip(sae_layers, saes):\n",
    "        # Get layer activations\n",
    "        layer_acts = cache['resid_pre', layer]\n",
    "        \n",
    "        # Get SAE activations\n",
    "        sae_acts = sae.encode(layer_acts)\n",
    "        \n",
    "        # Only keep activations for output tokens (last 50)\n",
    "        output_acts = sae_acts[:, -50:, :]\n",
    "        \n",
    "        all_layer_acts[layer] = output_acts.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "    return all_layer_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing perfect matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:02,  9.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing non-perfect matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:01, 11.54it/s]\n"
     ]
    }
   ],
   "source": [
    "if 'perfect_acts_stacked' in locals():\n",
    "    del perfect_acts_stacked\n",
    "if 'non_perfect_acts_stacked' in locals():\n",
    "    del non_perfect_acts_stacked\n",
    "\n",
    "# Get activations for perfect matches\n",
    "perfect_activations_by_layer = {layer: [] for layer in sae_layers}\n",
    "print(\"Processing perfect matches...\")\n",
    "for _, row in tqdm(perfect_sample.iterrows()):\n",
    "    acts_dict = get_activations(row['input_text'], row['generated_continuation_text'])\n",
    "    for layer in sae_layers:\n",
    "        perfect_activations_by_layer[layer].append(acts_dict[layer])\n",
    "\n",
    "# Get activations for non-perfect matches\n",
    "non_perfect_activations_by_layer = {layer: [] for layer in sae_layers}\n",
    "print(\"\\nProcessing non-perfect matches...\")\n",
    "for _, row in tqdm(non_perfect_sample.iterrows()):\n",
    "    acts_dict = get_activations(row['input_text'], row['generated_continuation_text'])\n",
    "    for layer in sae_layers:\n",
    "        non_perfect_activations_by_layer[layer].append(acts_dict[layer])\n",
    "\n",
    "# Stack all activations for each layer\n",
    "perfect_acts_stacked = {\n",
    "    layer: torch.stack(acts, dim=0) \n",
    "    for layer, acts in perfect_activations_by_layer.items()\n",
    "}\n",
    "non_perfect_acts_stacked = {\n",
    "    layer: torch.stack(acts, dim=0)\n",
    "    for layer, acts in non_perfect_activations_by_layer.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Activation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "def compute_stats(activations_dict):\n",
    "    \"\"\"Compute various statistics for the activations across all layers\"\"\"\n",
    "    stats_by_layer = {}\n",
    "    for layer, activations in activations_dict.items():\n",
    "        # L0 (number of active features)\n",
    "        l0 = (activations > 0).float().sum(-1).mean().item()\n",
    "        \n",
    "        # Mean activation when active\n",
    "        mean_active = activations[activations > 0].mean().item()\n",
    "        \n",
    "        # Max activation\n",
    "        max_act = activations.max().item()\n",
    "        \n",
    "        # Feature sparsity (fraction of features that never activate)\n",
    "        feature_sparsity = ((activations > 0).sum(0) == 0).float().mean().item()\n",
    "        \n",
    "        stats_by_layer[layer] = {\n",
    "            'L0 (avg active features)': l0,\n",
    "            'Mean activation when active': mean_active,\n",
    "            'Max activation': max_act,\n",
    "            'Feature sparsity': feature_sparsity\n",
    "        }\n",
    "    \n",
    "    return stats_by_layer\n",
    "\n",
    "perfect_stats = compute_stats(perfect_acts_stacked)\n",
    "non_perfect_stats = compute_stats(non_perfect_acts_stacked)\n",
    "\n",
    "# Print statistics for each layer\n",
    "for layer in sae_layers:\n",
    "    print(f\"\\nStatistics for layer {layer}:\")\n",
    "    print(\"Perfect matches:\")\n",
    "    for k, v in perfect_stats[layer].items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\nNon-perfect matches:\")\n",
    "    for k, v in non_perfect_stats[layer].items():\n",
    "        print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# Create DataFrames for plotting\n",
    "stats_data = []\n",
    "for layer in sae_layers:\n",
    "    # Perfect matches\n",
    "    for metric, value in perfect_stats[layer].items():\n",
    "        stats_data.append({\n",
    "            'Layer': layer,\n",
    "            'Metric': metric,\n",
    "            'Value': value,\n",
    "            'Type': 'Perfect Match'\n",
    "        })\n",
    "    # Non-perfect matches\n",
    "    for metric, value in non_perfect_stats[layer].items():\n",
    "        stats_data.append({\n",
    "            'Layer': layer,\n",
    "            'Metric': metric,\n",
    "            'Value': value,\n",
    "            'Type': 'Non-Perfect Match'\n",
    "        })\n",
    "\n",
    "stats_df = pd.DataFrame(stats_data)\n",
    "\n",
    "# Plot statistics across layers\n",
    "for metric in ['L0 (avg active features)', 'Mean activation when active', 'Max activation', 'Feature sparsity']:\n",
    "    metric_data = stats_df[stats_df['Metric'] == metric]\n",
    "    fig = px.line(metric_data, x='Layer', y='Value', color='Type',\n",
    "                 title=f'{metric} across Layers',\n",
    "                 labels={'Layer': 'Layer Number', 'Value': metric})\n",
    "    fig.show()\n",
    "\n",
    "# Plot L0 histograms for each layer\n",
    "for layer in sae_layers:\n",
    "    l0_perfect = (perfect_acts_stacked[layer] > 0).float().sum(-1).cpu().numpy()\n",
    "    l0_non_perfect = (non_perfect_acts_stacked[layer] > 0).float().sum(-1).cpu().numpy()\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    l0_data = pd.DataFrame({\n",
    "        'L0': np.concatenate([l0_perfect, l0_non_perfect]),\n",
    "        'Type': ['Perfect Match'] * len(l0_perfect) + ['Non-Perfect Match'] * len(l0_non_perfect),\n",
    "        'Layer': layer\n",
    "    })\n",
    "\n",
    "    # Plot histogram\n",
    "    fig = px.histogram(l0_data, x='L0', color='Type', barmode='overlay',\n",
    "                      title=f'Distribution of Active Features (L0) - Layer {layer}',\n",
    "                      labels={'L0': 'Number of Active Features', 'count': 'Frequency'})\n",
    "    fig.show()\n",
    "\n",
    "# Plot activation magnitude distributions for each layer\n",
    "for layer in sae_layers:\n",
    "    act_perfect = perfect_acts_stacked[layer][perfect_acts_stacked[layer] > 0].float().detach().cpu().numpy()\n",
    "    act_non_perfect = non_perfect_acts_stacked[layer][non_perfect_acts_stacked[layer] > 0].float().detach().cpu().numpy()\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    act_data = pd.DataFrame({\n",
    "        'Activation': np.concatenate([act_perfect, act_non_perfect]),\n",
    "        'Type': ['Perfect Match'] * len(act_perfect) + ['Non-Perfect Match'] * len(act_non_perfect),\n",
    "        'Layer': layer\n",
    "    })\n",
    "\n",
    "    # Plot histogram\n",
    "    fig = px.histogram(act_data, x='Activation', color='Type', barmode='overlay',\n",
    "                      title=f'Distribution of Activation Magnitudes - Layer {layer}',\n",
    "                      labels={'Activation': 'Activation Value', 'count': 'Frequency'})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate statistics\n",
    "# def compute_stats(activations):\n",
    "#     \"\"\"Compute various statistics for the activations\"\"\"\n",
    "#     # L0 (number of active features)\n",
    "#     l0 = (activations > 0).float().sum(-1).mean().item()\n",
    "    \n",
    "#     # Mean activation when active\n",
    "#     mean_active = activations[activations > 0].mean().item()\n",
    "    \n",
    "#     # Max activation\n",
    "#     max_act = activations.max().item()\n",
    "    \n",
    "#     # Feature sparsity (fraction of features that never activate)\n",
    "#     feature_sparsity = ((activations > 0).sum(0) == 0).float().mean().item()\n",
    "    \n",
    "#     return {\n",
    "#         'L0 (avg active features)': l0,\n",
    "#         'Mean activation when active': mean_active,\n",
    "#         'Max activation': max_act,\n",
    "#         'Feature sparsity': feature_sparsity\n",
    "#     }\n",
    "\n",
    "# perfect_stats = compute_stats(perfect_acts_stacked)\n",
    "# non_perfect_stats = compute_stats(non_perfect_acts_stacked)\n",
    "\n",
    "# # Print statistics\n",
    "# print(\"Statistics for perfect matches:\")\n",
    "# for k, v in perfect_stats.items():\n",
    "#     print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# print(\"\\nStatistics for non-perfect matches:\")\n",
    "# for k, v in non_perfect_stats.items():\n",
    "#     print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot L0 histograms\n",
    "# l0_perfect = (perfect_acts_stacked > 0).float().sum(-1).cpu().numpy()\n",
    "# l0_non_perfect = (non_perfect_acts_stacked > 0).float().sum(-1).cpu().numpy()\n",
    "\n",
    "# # Create a DataFrame for plotting\n",
    "# l0_data = pd.DataFrame({\n",
    "#     'L0': np.concatenate([l0_perfect, l0_non_perfect]),\n",
    "#     'Type': ['Perfect Match'] * len(l0_perfect) + ['Non-Perfect Match'] * len(l0_non_perfect)\n",
    "# })\n",
    "\n",
    "# # Plot histogram\n",
    "# fig = px.histogram(l0_data, x='L0', color='Type', barmode='overlay',\n",
    "#                   title='Distribution of Active Features (L0)',\n",
    "#                   labels={'L0': 'Number of Active Features', 'count': 'Frequency'})\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot activation magnitude distributions\n",
    "# act_perfect = perfect_acts_stacked[perfect_acts_stacked > 0].float().detach().cpu().numpy()\n",
    "# act_non_perfect = non_perfect_acts_stacked[non_perfect_acts_stacked > 0].float().detach().cpu().numpy()\n",
    "\n",
    "# # Create a DataFrame for plotting\n",
    "# act_data = pd.DataFrame({\n",
    "#     'Activation': np.concatenate([act_perfect, act_non_perfect]),\n",
    "#     'Type': ['Perfect Match'] * len(act_perfect) + ['Non-Perfect Match'] * len(act_non_perfect)\n",
    "# })\n",
    "\n",
    "# # Plot histogram\n",
    "# fig = px.histogram(act_data, x='Activation', color='Type', barmode='overlay',\n",
    "#                   title='Distribution of Activation Magnitudes',\n",
    "#                   labels={'Activation': 'Activation Value', 'count': 'Frequency'})\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare SAE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing layer 16:\n",
      "    Feature  Perfect  Non-Perfect\n",
      "4      6986      998          998\n",
      "7     11625      916          596\n",
      "2     10758      693          850\n",
      "6     14376      497          280\n",
      "9     24171      419          492\n",
      "3      9287      343          152\n",
      "5      6093      310          246\n",
      "8      6312      256          222\n",
      "1     26373      249          255\n",
      "11     1710      232          131\n",
      "0     21123      206          271\n",
      "10    10281      165          282\n",
      "12    19764      121          271\n",
      "\n",
      "Analyzing layer 17:\n",
      "    Feature  Perfect  Non-Perfect\n",
      "5     13218      933          954\n",
      "0      1282      806          517\n",
      "8     10800      615          584\n",
      "10    15289      459          434\n",
      "9       758      436          439\n",
      "4     10144      387          735\n",
      "6      1510      334          421\n",
      "2     31821      234          149\n",
      "1     14278      214          244\n",
      "3     14865      212          306\n",
      "7       745      198          432\n",
      "\n",
      "Analyzing layer 18:\n",
      "    Feature  Perfect  Non-Perfect\n",
      "4     25050      889          576\n",
      "11     2618      878          904\n",
      "8     27310      676          659\n",
      "0     13953      575          313\n",
      "3     29527      524          293\n",
      "9     13108      389          453\n",
      "6     25756      322          650\n",
      "10    10553      254          273\n",
      "5     21724      245          229\n",
      "7     21802      241          156\n",
      "1      3130      168          270\n",
      "2      8272      126          301\n",
      "\n",
      "Analyzing layer 19:\n",
      "    Feature  Perfect  Non-Perfect\n",
      "8     24820      685          658\n",
      "5     17504      518          554\n",
      "10    12155      484          306\n",
      "4       151      475          293\n",
      "0     16897      448          665\n",
      "3      1748      408          716\n",
      "7     18410      392          441\n",
      "9     17911      322          146\n",
      "6     12068      293          269\n",
      "1     15046      277          282\n",
      "2     23436      225          343\n"
     ]
    }
   ],
   "source": [
    "num_top_features = 10\n",
    "\n",
    "# Analyze feature usage patterns\n",
    "def get_top_features(activations, n=num_top_features):\n",
    "    \"\"\"Get the most frequently activated features\"\"\"\n",
    "    # Compress activations across all samples first\n",
    "    compressed_acts = activations.reshape(-1, activations.shape[-1])\n",
    "    feature_counts = (compressed_acts > 0).float().sum(0)\n",
    "    top_features = torch.topk(feature_counts, n)\n",
    "    return top_features.indices.cpu().numpy(), top_features.values.cpu().numpy()\n",
    "\n",
    "# Since we're working with a dict now, we need to handle one layer at a time\n",
    "aggregate_feature_comparisons_tables = {}\n",
    "for layer in sae_layers:\n",
    "    print(f\"\\nAnalyzing layer {layer}:\")\n",
    "    \n",
    "    # Get the activation tensors for this layer\n",
    "    perfect_acts = perfect_acts_stacked[layer]\n",
    "    non_perfect_acts = non_perfect_acts_stacked[layer]\n",
    "\n",
    "    # Get top features for both types\n",
    "    perfect_top_idx, perfect_top_counts = get_top_features(perfect_acts)\n",
    "    non_perfect_top_idx, non_perfect_top_counts = get_top_features(non_perfect_acts)\n",
    "\n",
    "    # Create a set of all features that were activated in either type\n",
    "    overlap_features = set(perfect_top_idx.tolist()) | set(non_perfect_top_idx.tolist())\n",
    "\n",
    "    # For each feature, see how many times it was activated in each type\n",
    "    perfect_acts_flat = perfect_acts.reshape(-1, perfect_acts.shape[-1])\n",
    "    non_perfect_acts_flat = non_perfect_acts.reshape(-1, non_perfect_acts.shape[-1])\n",
    "    \n",
    "    feature_counts = (perfect_acts_flat > 0).float().sum(0).int().cpu().numpy()\n",
    "    feature_counts_non_perfect = (non_perfect_acts_flat > 0).float().sum(0).int().cpu().numpy()\n",
    "    \n",
    "    # Create a table to show all the features (even some that we only activated by one type)\n",
    "    feature_counts_table = pd.DataFrame({\n",
    "        'Feature': list(overlap_features),\n",
    "        'Perfect': feature_counts[list(overlap_features)],\n",
    "        'Non-Perfect': feature_counts_non_perfect[list(overlap_features)]\n",
    "    })\n",
    "    \n",
    "    # # Add difference columns\n",
    "    # feature_counts_table['Abs_Difference'] = abs(feature_counts_table['Perfect'] - feature_counts_table['Non-Perfect'])\n",
    "    # feature_counts_table['Percent_Difference'] = (feature_counts_table['Abs_Difference'] / \n",
    "    #                                             feature_counts_table[['Perfect', 'Non-Perfect']].max(axis=1) * 100)\n",
    "    \n",
    "    # # Filter for absolute difference >= 100\n",
    "    # feature_counts_table = feature_counts_table[feature_counts_table['Abs_Difference'] >= 100]\n",
    "\n",
    "    # Sort the table by the number of activations in each type\n",
    "    feature_counts_table = feature_counts_table.sort_values(by=['Perfect', 'Non-Perfect'], ascending=False)\n",
    "\n",
    "    # Print the table\n",
    "    print(feature_counts_table)\n",
    "    aggregate_feature_comparisons_tables[layer] = feature_counts_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved feature counts for layer 16 to aggregate_top_feature_comparisons/feature_counts_layer_16.csv\n",
      "Saved feature counts for layer 17 to aggregate_top_feature_comparisons/feature_counts_layer_17.csv\n",
      "Saved feature counts for layer 18 to aggregate_top_feature_comparisons/feature_counts_layer_18.csv\n",
      "Saved feature counts for layer 19 to aggregate_top_feature_comparisons/feature_counts_layer_19.csv\n"
     ]
    }
   ],
   "source": [
    "# Create directory for feature comparison tables if it doesn't exist\n",
    "import os\n",
    "output_dir = \"aggregate_top_feature_comparisons\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for layer, feature_counts_table in aggregate_feature_comparisons_tables.items():\n",
    "    # Save the feature counts table for this layer\n",
    "    output_path = os.path.join(output_dir, f\"feature_counts_layer_{layer}.csv\")\n",
    "    feature_counts_table.to_csv(output_path, index=False)\n",
    "    print(f\"Saved feature counts for layer {layer} to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
